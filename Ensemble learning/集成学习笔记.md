# 集成学习

## 个体与集成

&emsp;&emsp;集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任务，也可称为多分类器系统(multi-classifier system)、基于委员会学习(committee-based learning)。

<center>![集成学习示意图](/Ensemble learning/image/EL.PNG)</center>
<center>**图1.1** <font face="微软雅黑" size = 2>集成学习示意图</font></center>

&emsp;&emsp;集成学习的一般结构：先产生一组“个体学习器”(indicidual learner)，再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等。

&emsp;&emsp;若集成只包含同种类型的个体学习器，例如“决策树集成”全是决策树，“神经网络集成”全是神经网络，这样的集成叫做“同质”(homogeneous)。同质集成中的个体学习器亦称“基学习器”(base learner)，相应的学习算法叫做“基学习算法”(base learning algorithm)。

&emsp;&emsp;若集成包含不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是“异质”的(heterogenous)。异质集成中的个体学习器由不同学习算法生成，这时就不再有基学习算法；相应的个体学习器一般不叫做基学习器，常称为“组件学习器”(component learner)或直接称为个体学习器。

&emsp;&emsp;集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”(weak learner)尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的，而基学习器有时也被直接称为弱学习器。但要注意的是，虽然理论上讲弱学习器集成足以获得好的性能，但实践中，出于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习器的一些经验等，人们往往会使用比较强的学习器。

&emsp;&emsp;一般经验中，把好坏不等的东西掺到一起，那么通常结果会比最坏的要好一些，比最好的坏一些。集成学习把多个学习器结合起来，如何获得比最好的单一学习器更好的性能呢？要想获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即个体学习器的性能不能太坏，并且要有“多样性”(diversity),即学习器间具有差异。

&emsp;&emsp;假设集成学习通过简单的投票法结合T个基分类器，若有超过半数的基分类器正确，则集成分类正确。又假设基分类器的错误率是互相独立。那么可以证明(利用Hoeffding不等式)随着集成中个体分类器数目T的增大，集成的错误率将指数下降，最终趋向0。

&emsp;&emsp;然而在现实任务中，个体学习器是为解决同一个问题训练出来的，它们显然不可能相互独立。事实上，个体学习器的“准确性”和“多样性”本身就存在冲突。一般准确性很高后，要想增加多样性就得牺牲准确性。集成学习研究的核心就是如何产生并结合“好而不同”的个体学习器。

&emsp;&emsp;根据个体学习器的生成方式，目前的集成学习方法大致可以分为两大类，即个体学习器间存在强依赖关系、必修串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可以同时生成的并行化方法；前者的代表是Boosting,后者的代表是Bagging和“随机森林”(Random Forest)。
