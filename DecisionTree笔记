决策树的一个重要任务是为了理解数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，
并从中提取出一系列规则，这些机器根据数据集创建规则的过程，就是机器学习的过程。

决策树的优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征
数据；缺点：可能会产生过度匹配问题；适用数据类型：数值型和标称型。

构造决策树时需要解决的第一个问题就是：当前数据集上的哪个特征在划分数据分类时起决定性作用。
为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征（特征评估的准则）。

创建分支的伪代码函数：
createBranch()
检测数据集中的每个子项是否属于同一分类：
If so return 类标签
Else
    寻找划分数据集的最好特征
    划分数据集
    创建分支节点
        for 每个划分的子集
            调用函数createBranch并增加返回结果到分支节点中
return 分支节点

决策树的一般流程
（1）收集数据：可以使用任何方法。
（2）准备数据：树构造算法只适合于标称型数据，因此数值型数据必须离散化。
（3）分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
（4）训练算法：构造树的数据结构。
（5）测试算法：使用经验树计算错误率。
（6）使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的
     内在含义。

信息增益
划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种
方法都有各自的优缺点。一种常用的方法就是使用信息论（entropy信息的期望值）度量信息。在划
分数据集之前之后信息发生变化称为信息增益，这样就可以计算每个特征值划分数据集获得的信息增
益，获得信息增益最高的特征就是最好的选择（因为信息量越大，对事物的描述越准确）。另外一个
度量集合无序程度的方法是基尼不纯度（Gini impurity），简而言之就是从一个数据集中随机选取
子项，度量其被错误分类到其他组的概率。
