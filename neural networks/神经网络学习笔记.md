# <center>神经网络</center>
## 6.1神经网络模型
<p1>神经网络是由具有适应性的简单单元组成的广泛并行互连网络，它的组织能够模拟生物神经系统对真实世界所作出的交互反应。神经网络中最基础的成分是神经元(neuron)模型，即定义中的“简单单元”。在生物神经网络中，每个神经元与其他神经元相连，当它兴奋，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位，如果某神经元的电位超过了一个“阈值”(threshold),那么它就会被激活，即兴奋，向其他神经元发送化学物质。1943年，McCulloch and Pitts 将上述情形抽象为如图6.1所示的简单模型:
<center>![Image of M-P model](/neural networks/image/M-P neuron-model.PNG)</center>
<center>**图6.1** <font face="微软雅黑" size = 2>M-P神经元模型</font></center>
在这个模型中，神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接(connection)进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”(activation function)处理以产生神经元的输出。</p1>
<p2>理想的激活函数是图6.2(a)所示的阶跃函数，它将输入值映射为输出这里的阶跃函数是"0"或"1"，显然 "1" 对应于神经元兴奋， "0" 对应于神经元抑制. 然而阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用 Sigmoid函数作为激活函数。
<center>![Image of activation function](/neural networks/image/activation-function.PNG)</center>
<center>**图6.2** <font face="微软雅黑" size = 2>典型的神经元激活函数</font></center></p2>
<p3>把许多这样的神经元按一定的层次结构连接起来，就是神经网络。从计算机科学的角度看，我们可以先不考虑神经网络是否真的模拟了生物神经网络，只需<font color=#FF0000>将一个神经网络视为包含了许多参数的数学模型</font>，这个模型是若干函数，例如![公式1](/neural networks/image/CodeCogsEqn.png)相互嵌套而得。有效的神经网络学习算法大多以数学证明为支撑。</p3>

## 6.2感知机与多层网络
<p4>感知机(Perceptron)由两层神经元组成，如图6.3所示，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，也叫“阈值逻辑单元”(threshold logic unit)。感知机能容易地实现逻辑与、或、非运算。
<center>![感知机网络结构示意图](/neural networks/image/Perceptron.PNG)</center>
<center>**图6.3** <font face="微软雅黑" size = 2>两个输入神经元的感知机网络结构示意图</font></center></p4>
<p5>更一般地，给定训练数据集，权重<font face = "Times New Roma">w<sub>i</sub>(i=1,2,...,n)</font>以及阈值theta可通过学习得到。阈值theta可看作一个固定输入为-1.0的“哑结点”(dummy node)所对应的连接权重w<sub>n+1</sub>,这样权重和阈值的学习就可统一为权重的学习。感知机学习规则非常简单，对训练样例(**x**,y),若当前感知机的输出为y<sup>'</sup>,则感知机权重将这样调整：
<center>![感知机权重调整公式](/neural networks/image/adjust-formula.PNG)</center>
其中eta属于(0,1)称为学习率(learning rate)。从上式可以看出，若感知机对训练样例预测正确，则感知机不发生变化，否则将根据错误的程度进行权重调整。**eta通常设置为一个小正数，例如0.1。** </p5>
<p6>需注意的是，感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元(function neuron),其学习能力非常有限。事实上，与、或、非问题都是线性可分(linearly separable)的问题。可以证明，若两类模式是线性可分的，即存在一个线性超平面能将它们分开，如图6.4(a)-(c)所示，则感知机的学习过程一定会收敛(converge)而求得适当的权向量**w** =(w<sub>1</sub>;w<sub>2</sub>;...;w<sub>n+1</sub>);否则感知机的学习过程将会发生振荡（fluctuation）,**w** 难以稳定，不能求得合适解，例如感知机甚至不能解决如图6.4(d)所示的异或这样简单的非线性可分问题。
<center>![线性可分问题和非线性可分问题示意图](/neural networks/image/linearly separable.PNG)</center>
<center>**图6.4** <font face="微软雅黑" size = 2>线性可分的“与”、“或”、“非”问题与非线性可分的“异或”问题</font></center></p6>
<p7>要解决非线性可分问题，需考虑使用多层功能神经元，例如图6.5中这个简单的两层感知机就能解决异或问题。在图6.5(a)中，输出层与输入层之间的一层神经元，称为隐含层(hidden layer)，隐含层和输出层神经元都是拥有激活函数的功能神经元。
<center>![两层感知机示意图](/neural networks/image/double-perceptron.PNG)</center>
<center>**图6.5** <font face="微软雅黑" size = 2>能解决异或问题的两层感知机</font></center></p7>
<p8>更一般的，常见的神经网络是形如图6.6所示的层级结构，每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接。这样的神经网络结构通常称为“多层前馈神经网络”(multi-layer feedforward neural networks),其中输入层神经元接收外界输入，隐含层与输出层神经元对信号进行加工，最终结果由输出层神经元输出；总的来说，输入层神经元仅是接受输入，不进行函数处理，隐含层与输出层包含功能神经元。神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”(connection weight)以及每个功能神经元的阈值；换言之，<font color=#FF0000>神经网络“学”到的东西，蕴含在连接权和阈值中</font>。
<center>![多层前馈神经网络结构示意图](/neural networks/image/multi-layer feedforward.PNG)</center>
<center>**图6.6** <font face="微软雅黑" size = 2>多层前馈神经网络结构示意图</font></center></p8>
