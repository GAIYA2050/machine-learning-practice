Logistic回归进行分类的主要思想：根据现有数据对分类边界线建立回归公式，以此进行分类。“回归”
一词源于最佳拟合，表示要找到最佳拟合参数集。训练分类器时的做法就是寻找最佳拟合参数，使用
的是最优化算法。

目标：找到一个好的方法去寻找最佳拟合参数。

Logistic回归的一般过程
（1）收集数据：采用任意方法收集数据。
（2）准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式最佳。
（3）分析数据：采用任意方法对数据分析。
（4）训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。
（5）测试算法：一旦训练完成，分类将会很快。
（6）使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；然后，基于训练好
的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，就可以在输
出的类别上做一些其他的分析工作。

Logistic回归的特点
优点：计算代价不高，易于理解和实现。
缺点：容易欠拟合，分类精度可能不高。
适用数据类型：数值型和标称型数据。

梯度上升法
梯度上升基于的思想是：要找到某个函数的最大值，最好的方法就是沿着该函数的梯度方向探寻。
梯度上升算法的迭代公式如下：w = w + alpha*f(w)的梯度 ，w是参数向量，alpha是步长。
梯度上升用于求函数的最大值，而梯度下降用于求函数的最小值。

训练算法：使用梯度上升找到最佳参数
伪代码如下：
每个回归系数初始化为1
重复R次：
  计算整个数据集的梯度
  使用alpha*gradient更新回归系数的向量
返回回归系数

训练算法：随机梯度上升
梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理100个左右的数据集问题
不大，但一旦有数十亿样本和成千上万的特征，那该方法的计算复杂度就太高了。一种改进的策略就
是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。由于可以在新样本来的时候
对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法，与“在线学习”相对应的概念
是批处理（一次处理所有的数据）。
伪代码：
所有回归系数初始化为1
对数据集中每个样本
  计算该样本的梯度
  使用alpha*gradient更新回归系数的向量
返回回归系数

示例：从疝气病症预测病马的死亡率
（1）收集数据：给定数据文件。
（2）准备数据：用Python解析文本文件并填充缺失值。
（3）分析数据：可视化并观察数据。
（4）训练算法：使用优化算法，找到最佳的系数。
（5）测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退训练阶段，通过
     改变迭代次数和步长等参数来得到更好的回归系数。
（6）使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事。

准备数据：处理数据中的缺失值
可选的做法
（1）使用可用特征的均值来填补缺失值；
（2）使用特殊值来填补缺失值，如-1（具体视数据集而定）；
（3）忽略有缺失值的样本；
（4）使用相似样本的均值添补缺失值；
（5）使用其他的机器学习算法预测缺失值。（半监督学习）

小结
Logistic回归的目的就是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法
完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算
法。随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。另外，随机梯度上升是
一个在线算法，它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批处理
运算。
机器学习的一个重要问题就是如何处理缺失数据，这个问题没有标准答案，取决于实际应用中的需求。
现有的一些解决方案各有优缺点，视情况选取。
