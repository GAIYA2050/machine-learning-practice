基于最大间隔分隔数据

支持向量机(Support Vector Machines, SVM)的特点
优点：泛化错误率低，计算开销不大，结果易解释。
缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。
适用的数据类型：数值型和标称型数据。

对于一个数据集，我们希望找到一个分隔超平面（即分类的决策边界，separating hyperplane）
在其分隔开，使之分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据都属于
另一个类别。采用这种方式构建的分类器，如果数据点离决策边界越远，那么其预测结果就越可信。
我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离足够远,（数据集中所有点到分隔面
的最小间隔的2倍称为分类器或者数据集的间隔margin）这是因为我们如果犯错或者在有限数据上训
练分类器的话，我们希望分类器尽可能健壮。
支持向量（support vector）就是离分隔超平面最近的那些点。
目标最大化支持向量到分隔面的距离，寻找此问题的优化解决方法。
支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量
有关（这是其计算开销低的缘故）。

SVM应用的一般框架
SVM的一般流程
（1）收集数据：可以使用任意方法。
（2）准备数据：需要数值型数据。
（3）分析数据：有助于可视化分隔超平面。
（4）训练算法：SVM的大部分时间都源自训练，该训练过程主要实现两个参数（w和b）的调优。
（5）测试算法：十分简单的计算过程就可以实现。
（6）使用算法：几乎所有分类问题都可以使用SVM，需要说明的是，SVM本身是一个二类分类器，对
多分类问题应用SVM需要对代码做一些修改。

Platt的SMO（序列最小化）算法
将大优化问题分解为多个小优化问题求解。这些小优化问题往往容易求解，并且对它们的顺序求解的
结果与将它们作为整体来求解的结果完全一致。在结果完全相同的同时，SMO算法的求解时间短很多。
SMO的基本思路是先固定alphai之外的所有参数，然后求alphai上的极值，但由于存在约束条件
sum(alphas) = 0，若固定alphai之外的其他变量，alphai可由其他变量导出，于是SMO每次选择
两个变量alphai和alphaj,并固定其他参数。
SMO算法的工作原理：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么增
大其中一个同时减小另一个。所谓合适就是指两个alpha必须符合一定条件，条件之一这两个alpha
必须在间隔边界之外，其二则是两个alpha还没有进行区间化处理或者不在边界上。
注意到只需选取的alphai和alphaj中有一个不满足KKT条件，目标函数就会在迭代后增大。直观来看，
KKT条件违背的程度越大，则变量更新后可能导致的目标函数值增幅越大。于是，SMO先选取违背KKT
条件程度最大的变量。第二个变量应该选择一个使目标函数值增长最快的变量，但由于比较各变量所
对应的目标函数值增幅的复杂度过高，因此SMO采用了一个启发式：使选取的两变量所对应样本之间
的间隔最大。一种直观的解释是，这样的两个变量有很大的差别，与对两个相似变量进行更新相比，
对它们进行更新会带给目标函数值更大的变化。


简易版SMO函数伪代码如下：
创建一个alpha向量并将其初始化为0向量
当迭代次数小于最大迭代次数时（外循环）：
  对数据集中的每个数据向量（内循环）：
    如果该数据向量可以被优化：
      随机选择另外一个数据向量
      同时优化这两个向量
      如果这两个向量都不能被优化，退出内循环
  如果所有向量都没被优化，增加迭代次数，继续下一次循环

核函数
之前的谈论中，我们假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类，然
而在实际问题中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面，例如简单的“异或”
问题就不是线性可分的。对于这种情况，可将样本从原始空间映射到一个更高维的特征空间，使得样
本在这个特征空间内线性可分。那么问题来了，是否一定存在这样的映射呢？幸运的是，有定理可以
保证：如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本线性可分
（指可以找到合适的划分超平面）
