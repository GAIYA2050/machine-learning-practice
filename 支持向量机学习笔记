基于最大间隔分隔数据

支持向量机(Support Vector Machines, SVM)的特点
优点：泛化错误率低，计算开销不大，结果易解释。
缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。
适用的数据类型：数值型和标称型数据。

对于一个数据集，我们希望找到一个分隔超平面（即分类的决策边界，separating hyperplane）
在其分隔开，使之分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据都属于
另一个类别。采用这种方式构建的分类器，如果数据点离决策边界越远，那么其预测结果就越可信。
我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离足够远,（数据集中所有点到分隔面
的最小间隔的2倍称为分类器或者数据集的间隔margin）这是因为我们如果犯错或者在有限数据上训
练分类器的话，我们希望分类器尽可能健壮。
支持向量（support vector）就是离分隔超平面最近的那些点。
目标最大化支持向量到分隔面的距离，寻找此问题的优化解决方法。

SVM应用的一般框架
SVM的一般流程
（1）收集数据：可以使用任意方法。
（2）准备数据：需要数值型数据。
（3）分析数据：有助于可视化分隔超平面。
（4）训练算法：SVM的大部分时间都源自训练，该训练过程主要实现两个参数（w和b）的调优。
（5）测试算法：十分简单的计算过程就可以实现。
（6）使用算法：几乎所有分类问题都可以使用SVM，需要说明的是，SVM本身是一个二类分类器，对
多分类问题应用SVM需要对代码做一些修改。

Platt的SMO（序列最小化）算法
将大优化问题分解为多个小优化问题求解。这些小优化问题往往容易求解，并且对它们的顺序求解的
结果与将它们作为整体来求解的结果完全一致。在结果完全相同的同时，SMO算法的求解时间短很多。
SMO算法的工作原理：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么增
大其中一个同时减小另一个。所谓合适就是指两个alpha必须符合一定条件，条件之一这两个alpha
必须在间隔边界之外，其二则是两个alpha还没有进行区间化处理或者不在边界上。

简易版SMO函数伪代码如下：
创建一个alpha向量并将其初始化为0向量
当迭代次数小于最大迭代次数时（外循环）：
  对数据集中的每个数据向量（内循环）：
    如果该数据向量可以被优化：
      随机选择另外一个数据向量
      同时优化这两个向量
      如果这两个向量都不能被优化，退出内循环
  如果所有向量都没被优化，增加迭代次数，继续下一次循环
