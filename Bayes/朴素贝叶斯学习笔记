基于贝叶斯决策理论的分类方法

朴素贝叶斯
优点：在数据较少的情况下仍然有效，可以处理多类别问题。
缺点：对于输入数据的准备方法比较敏感。
适用的数据类型：标称型数据。

kNN和决策树都要求分类器做出艰难选择，给出“该数据实例属于哪一类”这类问题的明确回答。但，
有时分类器会产生错误的结果，这时可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜
测的概率估计值。

贝叶斯决策理论的核心思想就是选择具有最高概率的决策。

朴素贝叶斯的一般过程
（1）收集数据：可以使用任何方法。
（2）准备数据：需要数值型或者布尔型数据。
（3）分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更佳。
（4）训练算法：计算不同的独立特征的条件概率。
（5）测试算法：计算错误率。
（6）使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的场景中使用朴素贝叶斯分类
器，不一定非要是文本。

朴素贝叶斯分类器的一个假设是，每个特征同等重要。朴素贝叶斯分类器通常有两种实现方式：一种
基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式，该实现方式并不考虑词
在文档中出现的次数，只考虑出不出现，因此这个意义相当于假设词是等权重的。另外一个假设就是
每个特征互相独立，这样p(x,y | ci)可以用p(x|ci)*p(y|ci)来计算。实际中样本有限，可能根据
样本计算出的p(wi|ci)为0，那么最后的乘积也为0，为了降低这种影响，可以将所有词出现数初始化
为1，并将分母初始化为2.

p(c1 | x,y)表示给定某个由x,y表示的数据点，那么该数据点来自类别c1的概率
p(x,y | c1)表示给定类别c1，那么该数据点是由x,y表示的概率
贝叶斯准则：p(ci | x,y) = p(x,y | ci)*p(ci)/p(x,y)
贝叶斯分类准则：
若p(c1 | x,y) > p(c2 | x,y),那么属于类别c1;
若p(c1 | x,y) < p(c2 | x,y),那么属于类别c2.

训练算法：从词向量计算概率
伪代码如下：
计算每个类别中的文档数目
对每篇训练文档：
  对每个词条：
    如果词条出现在文档中>>增加该词条的计数值
    增加所有词条的计数值
对每个类别：
  对每个词条：
    将该词条的数目除以总词条数目得到条件概率
返回每个类别的条件概率

示例：使用朴素贝叶斯对电子邮件进行分类
（1）收集数据：提供文本文件。
（2）准备数据：将文本文件解析成词条向量。
（3）分析数据：检查词条确保解析的正确性。
（4）训练算法：使用我们建立的trainNBO()函数。
（5）测试算法：使用classifyNB(),并且构建一个新的测试函数来计算文档集的错误率。
（6）使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。

示例：使用朴素贝叶斯来发现地域相关的用词
（1）收集数据：从RSS源收集内容，这里需要对RSS源构建一个接口。
（2）准备数据：将文本文件解析成词条向量。
（3）分析数据：检查词条确保解析的正确性。
（4）训练算法：使用我们建立的trainNBO()函数。
（5）测试算法：观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果。
（6）使用算法：构建一个完整的程序，封装所有内容。给定两个RSS源，该程序会显示最常用的公共词。

总结：对于分类而言，使用概率有时要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种
利用已知值来估计未知概率的有效方法。
可以通过特征之间的条件独立性假设，降低对数据量的需求。独立性假设是指一个词的出现概率并不
依赖于文档中的其他词。当然这个假设过于简单，这就是之所以称为朴素贝叶斯的原因。尽管条件独
立性假设并不正确，但朴素贝叶斯仍然是一种有效的分类器。
利用现代编程语言来实现朴素贝叶斯时需要考虑很多实际因素。下溢出就是其中一个问题，它可以通
过对概率取对数来解决。词袋模型在解决文档分类问题上比词集模型有所提高。
